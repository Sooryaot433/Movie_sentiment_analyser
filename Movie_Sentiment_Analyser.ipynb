{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USVYb6-GvclZ",
        "outputId": "3c7fea03-a7a7-457b-8035-be3fa4d8883a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install --upgrade datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "from collections import Counter\n",
        "import math\n",
        "dataset=load_dataset(\"imdb\")\n",
        "train_data=dataset[\"train\"]\n",
        "test_data=dataset[\"test\"]\n",
        "def clean_text(text):\n",
        "  text=text.lower()\n",
        "  text=re.sub(r'<[^>]+>','',text)\n",
        "  text=re.sub(r'[^\\w\\s]','',text)\n",
        "  text=re.sub(r'\\d+','',text)\n",
        "  return text.strip()\n",
        "train_texts = [clean_text(x['text']) for x in train_data]\n",
        "train_labels = [x['label'] for x in train_data]\n",
        "test_texts = [clean_text(x['text']) for x in test_data]\n",
        "test_labels = [x['label'] for x in test_data]\n",
        "class Tokenizer:\n",
        "    def __init__(self, max_words=20000):\n",
        "        self.vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3}\n",
        "        self.max_words = max_words\n",
        "    def fit(self, texts):\n",
        "        word_counts = Counter()\n",
        "        for text in texts:\n",
        "            word_counts.update(text.split())\n",
        "        for word, _ in word_counts.most_common(self.max_words - 4):\n",
        "            self.vocab[word] = len(self.vocab)\n",
        "    def encode(self, text):\n",
        "        return [self.vocab.get(word, 1) for word in text.split()]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit(train_texts)\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "class TransformerModel:\n",
        "    def __init__(self, vocab_size, embed_size=128):\n",
        "        self.embeddings = np.random.randn(vocab_size, embed_size) * 0.01\n",
        "        self.Wq = np.random.randn(embed_size, embed_size) * 0.01\n",
        "        self.Wk = np.random.randn(embed_size, embed_size) * 0.01\n",
        "        self.Wv = np.random.randn(embed_size, embed_size) * 0.01\n",
        "        self.fc = np.random.randn(embed_size, 2) * 0.01\n",
        "\n",
        "    def attention(self, x):\n",
        "        Q = np.dot(x, self.Wq)\n",
        "        K = np.dot(x, self.Wk)\n",
        "        V = np.dot(x, self.Wv)\n",
        "\n",
        "        scores = np.dot(Q, K.T) / np.sqrt(x.shape[-1])\n",
        "        attention = softmax(scores)\n",
        "        return np.dot(attention, V)\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        x = self.embeddings[token_ids]\n",
        "        x = x.mean(axis=0)\n",
        "        return softmax(np.dot(x, self.fc))\n",
        "    def train(self, texts, labels, epochs=3, lr=0.01):\n",
        "        for epoch in range(epochs):\n",
        "            correct = 0\n",
        "            for i, (text, label) in enumerate(zip(texts, labels)):\n",
        "                tokens = tokenizer.encode(text)\n",
        "                probs = self.forward(tokens)\n",
        "                pred = np.argmax(probs)\n",
        "                if pred != label:\n",
        "                    grad = probs - np.array([1-label, label])\n",
        "                    self.fc -= lr * np.outer(self.embeddings[tokens].mean(0), grad)\n",
        "\n",
        "                correct += (pred == label)\n",
        "\n",
        "                if i % 1000 == 0:\n",
        "                    print(f\"Batch {i}, Accuracy: {correct/(i+1):.2%}\")\n",
        "\n",
        "            print(f\"Epoch {epoch+1}, Final Accuracy: {correct/len(texts):.2%}\")\n",
        "\n",
        "\n",
        "model = TransformerModel(vocab_size)\n",
        "model.train(train_texts[:2000], train_labels[:2000])\n",
        "def evaluate(model, texts, labels):\n",
        "    correct = 0\n",
        "    for text, label in zip(texts, labels):\n",
        "        tokens = tokenizer.encode(text)\n",
        "        probs = model.forward(tokens)\n",
        "        correct += (np.argmax(probs) == label)\n",
        "    print(f\"Test Accuracy: {correct/len(texts):.2%}\")\n",
        "\n",
        "evaluate(model, test_texts[:500], test_labels[:500])\n",
        "\n",
        "def predict(text):\n",
        "    tokens = tokenizer.encode(clean_text(text))\n",
        "    probs = model.forward(tokens)\n",
        "    sentiment = \"Positive\" if np.argmax(probs) == 1 else \"Negative\"\n",
        "    confidence = np.max(probs)\n",
        "    print(f\"Text: {text[:50]}...\")\n",
        "    print(f\"Sentiment: {sentiment} ({confidence:.2%} confidence)\")\n",
        "\n",
        "predict(\"This movie was absolutely wonderful!\")\n",
        "predict(\"I hated every minute of this terrible film.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEWdeeT3zSmL",
        "outputId": "2d7fe009-53a6-48a7-a689-0b6b9ed64f57"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 20000\n",
            "Batch 0, Accuracy: 100.00%\n",
            "Batch 1000, Accuracy: 96.70%\n",
            "Epoch 1, Final Accuracy: 97.10%\n",
            "Batch 0, Accuracy: 100.00%\n",
            "Batch 1000, Accuracy: 98.70%\n",
            "Epoch 2, Final Accuracy: 98.65%\n",
            "Batch 0, Accuracy: 100.00%\n",
            "Batch 1000, Accuracy: 98.90%\n",
            "Epoch 3, Final Accuracy: 98.85%\n",
            "Test Accuracy: 98.80%\n",
            "Text: This movie was absolutely wonderful!...\n",
            "Sentiment: Positive (50.01% confidence)\n",
            "Text: I hated every minute of this terrible film....\n",
            "Sentiment: Negative (50.01% confidence)\n"
          ]
        }
      ]
    }
  ]
}